---
layout: home
order: 1
permalink: /
title: About
desc_title: SyntaGen - Harnessing Generative Models for Synthetic Visual Datasets
description:  <strong>CVPR 2024 Workshop <br> June 17th, Morning <br> Seattle, United States</strong>
social: true
---
The field of computer vision has undergone a significant transformation in recent
years with the advancement of generative models, particularly text-to-image models
such as Imagen, Stable Diffusion, and DALLE-3. These models have enabled the
creation of synthetic visual datasets that are highly realistic and diverse, complete
with annotations and rich variations. These datasets have proven to be extremely
valuable in training and evaluating various computer vision algorithms, including
object detection and segmentation, representation learning, and scene understanding. The SyntaGen workshop will act as a crucible for an inclusive exchange of
ideas, practical insights, and collaborative explorations. By convening experts and
enthusiasts from various corners of the field, it strives to propel the development of
generative models and synthetic visual datasets to new heights. Through informative talks, challenges, poster sessions, paper presentations, and vibrant panel discussions, this workshop endeavors to lay the foundation for innovative breakthroughs that bridge the realms of generative models and computer vision applications.
<!-- **UPDATE**: fill out this form if you are interested in a post-workshop social: [https://forms.gle/XjeSVmyHnsp7EmLB6](https://forms.gle/XjeSVmyHnsp7EmLB6). -->

<!-- ### Schedule (Meeting Room 317A, 9 AM - 5 PM, July 29, 2023) -->
<!-- ### Schedule

⭐ **Link to NeurIPS page: [https://neurips.cc/virtual/2023/workshop/66550](https://neurips.cc/virtual/2023/workshop/66550)** ⭐


|----------------------|---------------------------------------------------------|---------------------------------------------------------------------------------------|
| Start Time (CST/GMT-06:00, New Orleans)  |  Session                                                 | Speaker(s)                                                                            |
|:---------------------|:--------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------|
| 08:55 am | Opening Remarks                                                                            | Organizers                                                                            |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 09:00 am | **Invited Talk 1:** A Blessing in Disguise: Backdoor Attacks as Watermarks for Dataset Copyright | Yiming Li |
| 09:30 am | **Invited Talk 2:** Recent Advances in Backdoor Defense and Benchmark | Baoyuan Wu  |
| 10:00 am | Coffee Break                                                                           |  |
|---------------------|--------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 10:30 am | **Invited Talk 3:** The difference between safety and security for watermarking                                                                                | Jonas Geiping |
| 11:00 am | **Oral 1:** Effective Backdoor Mitigation Depends on the Pre-training Objective | Sahil Verma, Gantavya Bhatt, Soumye Singhal, Arnav Das, Chirag Shah, John Dickerson, Jeff A Bilmes |
| 11:15 am  | **Invited Talk 4:** Universal jailbreak backdoors from poisoned human feedback | Florian Tramèr |
| 11:45 am | Lunch Break | |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 01:00 pm | **Oral 2:** VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models | Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho |
| 01:15 pm | **Oral 3:** The Stronger the Diffusion Model, the Easier the Backdoor: Data Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline | Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi |
| 01:30 pm | **Invited talk 5:** Is this model mine? On stealing and defending machine learning models | Adam Dziedzic |
| 02:00 pm | **Invited talk 6**                                                                           | Ruoxi Jia |
| 02:30 pm | Coffee Break                                                                     |  |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 03:00 pm | **Poster Session**                                                                                | Paper Authors |
| 03:45 pm | **Oral 4:** Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection | Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin |
| 04:00 pm | **Oral 5:** BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models | Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha Poovendran, Bo Li |
| 04:15 pm | **Invited Talk 7:** Decoding Backdoors in LLMs and Their Implications | Bo Li |
|---------------------|----------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| 04:45 pm | **Panel Discussion**                                                                     | Moderator: Eugene Bagdasaryan |
| 05:15 pm   | Closing Remarks                                                                        | Organizers    |  -->


### **Speakers**

<table style="width:100%">
  <tr>
    <td style="text-align:center border-radius:50%">
      <a href="https://www.cs.toronto.edu/~fleet/"><img src="assets/img2/david.jpg" style="border-radius:50%;" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://web.mit.edu/phillipi/"><img src="assets/img2/philip_isola.jpg" style="border-radius:50%;" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://jbhuang0604.github.io/"><img src="assets/img2/jinbin2.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://www.weizmann.ac.il/math/dekel/"><img src="assets/img2/TaliDekel_w4.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://research.adobe.com/person/nathan-carr/"><img src="assets/img2/nathan.png" height="150" width="150" style="border-radius:50%;"></a>
    </td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://www.cs.toronto.edu/~fleet/">David J Fleet</a><br>Professor<br>University of Toronto, Google DeepMind</td>
    <td style="text-align:center"><a href="https://web.mit.edu/phillipi/">Phillip Isola</a><br>Associate Professor<br>MIT</td>
    <td style="text-align:center"><a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a><br>Associate Professor<br>University of Maryland College Park</td>
    <td style="text-align:center"><a href="https://www.weizmann.ac.il/math/dekel/">Tali Dekel</a><br>Assistant Professor<br>Weizmann Institute of Science</td>
    <td style="text-align:center"><a href="https://research.adobe.com/person/nathan-carr/">Nathan Carr</a><br>Research Scientist<br>Adobe Fellow</td>
  </tr>
</table>

### **Panelists**
* TBD

### **Schedule**
* TBD

### **Accepted Papers**
* TBD

### **Call for Papers**
We invite papers to propel the development of generative models and/or the use of their synthetic visual datasets for training and evaluating computer vision models. Accepted papers will be presented in the poster session in our workshop. We welcome submissions along two tracks:
* **Full papers**: Up to 8 pages, excluding references, with option for inclusion in the proceedings.
* **Short papers**: Up to 4 pages, excluding references, not for the proceedings. 

Only full papers will be considered for the Best Paper award. Additionally, we offer a Best Paper and a Best Paper Runner-up award with oral presentations. All accepted papers without inclusion in the proceedings are non-archival.

#### Topics
The main objective of the SyntaGen workshop is to offer a space for researchers, practitioners, and enthusiasts to investigate, converse, and cooperate on the development, use, and potential uses of synthetic visual datasets made from generative models. The workshop will cover various topics, including but not restricted to:
* Leveraging pre-trained generative models to generate data and annotations for perception-driven tasks, including image classification, object detection, semantic and instance segmentation, relationship detection, action recognition, object tracking, and 3D shape reconstruction and recognition.
* Extending the generative capacity of large-scale pre-trained text-to-image models to other domains, such as videos and 3D spaces.
* Synergizing expansive synthetic datasets with minimally annotated real datasets to enhance model performance across scenarios including unsupervised, semi-supervised, weakly-supervised, and zero-shot/few-shot learning.
* Exploring generative model learning from small-scale datasets, paving the way for effective data generation when faced with limited training data.
* Enhancing data quality and improving synthesis methodologies in the context of pre-trained text-to-image (T2I), text-to-video (T2V), and text-to-3D models.
* Evaluating the quality and effectiveness of the generated datasets, particularly on metrics, challenges, and open problems related to benchmarking synthetic visual datasets.
* Ethical implications of using synthetic annotated data, strategies for mitigating biases, and ensuring responsible data generation and annotation practices.

#### Submission Instructions
Submissions should be anonymized and formatted using the [CVPR 2024 template](https://github.com/cvpr-org/author-kit/releases) and uploaded as a single PDF.

#### Supplementary material
Supplemental materials optionally can be submitted along the paper manuscript on the submission deadline. They must be anonymized and uploaded either as a single PDF or a ZIP file.

#### Submission link
[Openreview submission link](https://openreview.net/group?id=thecvf.com/CVPR/2024/Workshop/SyntaGen )

#### Important workshop dates
* Submission deadline: **March 22nd, 11:59 PM Pacific Time**
* Notification of acceptance: **April 7th, 11:59 PM Pacific Time**
* Camera Ready submission deadline: **April 14th, 11:59 PM Pacific Time**
* Workshop date: **June 17th, 2024 (Morning)**

#### Contact us
If you have any further questions, please feel free to contact us at **cvpr24syntagen@googlegroups.com**

### **SyntaGen Competition**
<div class="image-container" style="max-width: 100%; text-align: center;">
  <img src="./assets/img2/dataset-diff-teaser.png" style="width: 100%; height: auto;">
</div>

#### Dataset and metric
The primary objective of this competition is to drive innovation in the creation of high-quality synthetic datasets, leveraging only the pretrained [Stable Diffusion](https://huggingface.co/spaces/stabilityai/stable-diffusion) and the 20 class names from [PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/) for semantic segmentation. The evaluation of synthetic dataset quality involves training a [DeepLabv3](https://arxiv.org/abs/1706.05587) model on the synthetic dataset and subsequently assessing its performance on a private test set on the task of semantic segmentation  (a sample validation set is the validation set of [PASCAL VOC 2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/)). Submissions are ranked based on the **mIoU** metric. This competition framework mirrors the practical application of synthetic datasets, particularly in scenarios where they replace real datasets.

#### Requirements
Teams are required to upload their synthetic datasets to **Google Drive** for public access. The dataset should include a maximum of **10K** pairs of synthesized images and semantic segmentations, each image being **512x512** in size. We will supply a [Google Colab](https://colab.research.google.com/drive/1kizZ0Ix2SNP11qy_VMhr0NLGlI5-oMNT) codebase for exemplifying some random training images and their annotations, DeepLabv3 training and evaluation, using a **Resnet-50** backbone, and showing qualitative results. Training and evaluation time for DeepLabv3 is capped at **10K iterations** with a **batch size of 8**.  The codebase should remain unchanged except for the Google Drive file ID provided by each team. For text prompts, participants can leverage various methods, including LLMs like [ChatGPT](https://chat.openai.com/). For text-to-image generation, participants can choose either **Stable Diffusion (v1.x) or (v2.x)**. However, participants cannot use any additional segmentation datasets. One approach can be [Dataset Diffusion](https://arxiv.org/abs/2309.14303), and the source code for which is accessible at [Github](https://github.com/VinAIResearch/Dataset-Diffusion/tree/main).

#### Submission and evaluation
The submission comprises two phases with two separate deadlines,
outlined as follows:

1.  **Random seed + DeepLabv3 + Dataset**. By the first deadline in this phase, each
team must use their method to generate their synthetic dataset and train a DeepLabv3 on
it. Each team must submit:
* The **random seed** used in their model for dataset generation, 
* The **generated dataset** 
* The **checksum of the trained DeepLabv3**. The checksum
code will be provided as part of the Colab codebase. Modifications to the model, the trained
DeepLabv3, or the generated dataset are prohibited after this deadline.

2.  **Code + Score**. After the first deadline, our private test set will be released to each team.
Each team must evaluate their trained DeepLabv3 on the test set and submit their code and
mIoU score on the same Colab file as in the previous deadline by the final deadline.

Entries will be evaluated based on their mIoU, and the top submissions will be contacted for further
verification. These selected participants will be required to share their code (via **GitHub**) to replicate
the synthetic datasets (consisting of images and annotations) using the specified random seed and
checksum. Additionally, the provided Google Colab file will be employed to reproduce both the
training and inference processes of DeepLabv3, ensuring accurate results and confirming adherence
to Stable Diffusion as the exclusive text-to-image generator. Any submissions found to
be non-compliant or in violation of the rules will be disqualified. In such instances, the evaluation
process will continue with the subsequent submissions until the final top 2 are determined

#### Submission Form:
* TBD

#### Google Colab file training and evaluating synthetic dataset
Please use this [Google Colab file](https://colab.research.google.com/drive/1kizZ0Ix2SNP11qy_VMhr0NLGlI5-oMNT) for training and evaluating your synthetic dataset. 
<br>
**Note:** You can run it in Google Colab or download it as a Jupyter notebook to run on your local machine (advised).


#### Prizes and presentation: 
We will award the top 2 teams with cash prizes **(Rank 1: $1000, Rank 2: $500)** and invite them to write a report and present their work at the workshop **(10 minutes each)**.

<!-- ### Ethical considerations for the datasets
Since the evaluated dataset is the validation set of PASCAL VOC 2012, it shares the same ethical considerations with PASCAL VOC 2012. Besides, the generated
synthetic training set has new considers including

 * **Data Bias and Fairness**: The creation of synthetic datasets through Stable Diffusion
can introduce unintended biases, potentially deviating from real-world data characteristics.
Ethical vigilance is essential to identify and rectify any biases, ensuring that the synthetic
datasets remain representative, unbiased, and fair.

* **Privacy and Consent**: Although synthetic datasets do not involve real individuals’ data,
traces of underlying characteristics might inadvertently capture sensitive information. Up-holding ethical principles necessitates the thorough anonymization or removal of such data
traces to protect privacy and respect consent standards. -->

#### Important Dates

* Competition announcement: **Feb 22nd, 2024**
* Submission start: **Mar 1st, 2024**
* Submission deadline 1 for random seed, DeepLabv3, and dataset: **May 24th, 2024**
* Submission deadline 2 for dataset generation code and mIoU score: **May 27th, 2024**
* Award announcement: **Jun 7th, 2024**
* Report and code upload for winners: **Jun 14th, 2024**

#### Discussion community
Let’s join SyntaGen’s community in <a href="https://discord.gg/QuPgtx78kg" target="_blank">**Discord**</a> to discuss anything related to the challenge

### **Workshop Sponsors**
<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.vinai.io/"><img src="assets/img/inst-vinai.png" height="60"></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://research.adobe.com/"><img src="assets/img2/aff-adobe.jpg" height="70"></a></td>
  </tr>
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.vinai.io/"><b>VinAI</b></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.adobe.com/"><b>Adobe</b></a></td>
  </tr>
</table>

### **Organizers**
<table style="width:100%">
  <tr>
    <td style="text-align:center border-radius:50%">
      <a href="https://khoinguyen.org"><img src="assets/img2/org-khoinguyen.jpg" style="border-radius:50%;" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&hl=en"><img src="assets/img/org-anh-tran-square.jpg" style="border-radius:50%;" height="150" width="150"></a>
    </td>
    <td style="text-align:center">
      <a href="https://sonhua.github.io/"><img src="assets/img2/org-sonhua.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://www.supasorn.com/"><img src="assets/img2/org-supasorn.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://zhouyisjtu.github.io/"><img src="assets/img2/org-yizhou.png" height="150" width="150" style="border-radius:50%;"></a>
    </td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://khoinguyen.org">Khoi Nguyen</a> <br>VinAI Research, Vietnam</td>
    <td style="text-align:center"><a href="https://scholar.google.com/citations?user=FYZ5ODQAAAAJ&hl=en">Anh Tuan Tran</a> <br>VinAI Research, Vietnam</td>
    <td style="text-align:center"><a href="https://sonhua.github.io/">Binh Son Hua</a><br>Trinity College Dublin, Ireland</td>
    <td style="text-align:center"><a href="https://www.supasorn.com/">Supasorn Suwajanakorn</a> <br>VISTEC, Thailand</td>
    <td style="text-align:center"><a href="https://zhouyisjtu.github.io/">Yi Zhou</a><br>Adobe</td>
  </tr>
</table>

### **Volunteers**
<table style="width:100%">
  <tr>
    <td style="text-align:center">
      <a href="https://truongvu2000nd.github.io/"><img src="assets/img2/truong.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
    <td style="text-align:center">
      <a href="https://quang-ngh.github.io/"><img src="assets/img2/quang.jpg" height="150" width="150" style="border-radius:50%;"></a>
    </td>
  </tr>
  <tr>
    <td style="text-align:center"><a href="https://truongvu2000nd.github.io/">Truong Vu</a><br>Research Resident<br>VinAI Research, Vietnam</td>
    <td style="text-align:center"><a href="https://quang-ngh.github.io/">Quang Nguyen</a><br>Research Intern<br>VinAI Research, Vietnam</td>
  </tr>
</table>


### **Organizers affiliations**
<!-- <td style="text-align:center"><a href="https://vinuni.edu.vn/college-of-engineering-computer-science/"><img src="assets/img/inst-vinuni-cecs.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.cs.umd.edu/"><img src="assets/img/inst-umd-cs.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.vinai.io/"><img src="assets/img/inst-vinai.png" height="75"></a></td>
<br>


<td style="text-align:center"><a href="https://www.clemson.edu/index.html"><img src="assets/img/inst-clemson.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.deakin.edu.au/"><img src="assets/img/inst-deakin.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://tech.cornell.edu/"><img src="assets/img/inst-cornell-tech.png" height="75"></a></td>
<br>

<td style="text-align:center"><a href="https://www.nyu.edu/"><img src="assets/img/New_York_University-Logo.png" height="75"></a></td> -->

<table style="width:100%; align: left; border: none; spacing: none">
  <tr style="border: none; spacing: none"> 
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.vinai.io/"><img src="assets/img/inst-vinai.png" height="60"></a></td>  
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.tcd.ie/"><img src="assets/img2/aff-tcd.png" height="80"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://www.vistec.ac.th/"><img src="assets/img2/aff-vistec.jpeg" height="70"></a></td>
    <td style="text-align:center; border: none; spacing: none"><a href="https://research.adobe.com/"><img src="assets/img2/aff-adobe.jpg" height="70"></a></td>
  </tr>
</table>