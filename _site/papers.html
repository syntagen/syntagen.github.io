<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>SyntaGen | Call for Papers</title>
  <meta name="description" content="Research on Generative Models for Synthetic Visual Datasets
">
  <meta property="og:image" content="https://syntagen.github.io/assets/img2/workshop-thumbnail.jpg" />

  <link rel="shortcut icon" href="http://localhost:4000/assets/img/favicon.ico">

  <link rel="stylesheet" href="http://localhost:4000/assets/css/main.css">
  <link rel="canonical" href="http://localhost:4000/papers">
  <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/base-min.css">
</head>


  <body>

    <header class="site-header">


  <div class="wrapper">
    

    <a href="http://localhost:4000">
    <span class="site-title">
        
        <!--<strong>SyntaGen</strong>-->
        SyntaGen
    </span>
    </a>

    <!-- <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        
        
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="http://localhost:4000/">About</a>
          
        
          
            <a class="page-link" href="http://localhost:4000/schedule">Schedule</a>
          
        
          
            <a class="page-link" href="http://localhost:4000/papers">Call for Papers</a>
          
        
          
            <a class="page-link" href="http://localhost:4000/challenge">Challenge</a>
          
        


      </div>
    </nav> -->

  </div>
  <meta name="robots" content="noindex">

</header>



    <div class="page-content">
      
      <div class="header-background"><div class="img"></div></div>
      <div class="wrapper-button" style="display: flex; justify-content: center; flex-wrap: wrap; gap: 10px;">
        <a href="./" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
          Home
        </a>
        <a href="#speakers" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
          Speakers
        </a>
        <a href="#schedule" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
          Schedule
        </a>
        <a href="#call-for-papers" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
          Call for Papers
        </a>
        <a href="#syntagen-competition" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
          Challenge
        </a>
        <a href="#organizers" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease;">
          Organizers
        </a>
        <a href="#syntagen-2025" style="display: inline-block; padding: 10px 20px; background-color: #4a90e2; color: white; text-decoration: none; border-radius: 5px; box-shadow: 0px 4px 6px rgba(0, 0, 0, 0.1); transition: all 0.3s ease; position: relative;">
          SyntaGen 2025
          <span style="display: inline-block; background-color: #ff5252; color: white; font-size: 12px; padding: 2px 5px; border-radius: 3px; position: absolute; top: -5px; right: -10px;">
            New
          </span>
        </a>
      </div>      
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Call for Papers</h1>
    <h3 class="post-description"></h3>
  </header>

  <article class="post-content Call for Papers clearfix">
    <p>We invite papers to propel the development of generative models and/or the use of their synthetic visual datasets for training and evaluating computer vision models. Accepted papers will be presented in the poster session in our workshop. Additionally, we offer a Best Paper and a Best Paper Runner-up award with oral presentations.</p>

<h3 id="topics">Topics</h3>
<p>The main objective of the SyntaGen workshop is to offer a space for researchers, practitioners, and enthusiasts to investigate, converse, and cooperate on the development, use, and potential uses of synthetic visual datasets made from generative models. The workshop will cover various topics, including but not restricted to:</p>
<ul>
  <li>Leveraging pre-trained generative models to generate data and annotations for perception-driven tasks, including image classification, object detection, semantic and instance segmentation, relationship detection, action recognition, object tracking, and 3D shape reconstruction and recognition.</li>
  <li>Extending the generative capacity of large-scale pre-trained text-to-image models to other domains, such as videos and 3D spaces.</li>
  <li>Synergizing expansive synthetic datasets with minimally annotated real datasets to enhance model performance across scenarios including unsupervised, semi-supervised, weakly-supervised, and zero-shot/few-shot learning.</li>
  <li>Exploring generative model learning from small-scale datasets, paving the way for effective data generation when faced with limited training data.</li>
  <li>Enhancing data quality and improving synthesis methodologies in the context of pre-trained text-to-image (T2I), text-to-video (T2V), and text-to-3D models.</li>
  <li>Evaluating the quality and effectiveness of the generated datasets, particularly on metrics, challenges, and open problems related to benchmarking synthetic visual datasets.</li>
  <li>Ethical implications of using synthetic annotated data, strategies for mitigating biases, and ensuring responsible data generation and annotation practices.</li>
</ul>

<h3 id="submission-instructions">Submission Instructions</h3>
<p>We invite contributions in the form of full papers (up to 8 pages, excluding references, for inclusion in the proceedings) or short papers (up to 4 pages, excluding references, not for the proceedings). Only full papers will be considered for the Best Paper award. Submissions should be anonymized and formatted using the <a href="https://github.com/cvpr-org/author-kit/releases">CVPR 2024 template</a> and uploaded as a single PDF. Note that our workshop is non-archival.</p>

<h4 id="supplementary-material">Supplementary material</h4>
<p>Supplemental materials optionally can be submitted along the paper manuscript on the submission deadline. They must be anonymized and uploaded either as a single PDF or a ZIP file.</p>

<h4 id="submission-link">Submission link</h4>
<p><a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Workshop/SyntaGen">Link</a></p>

<h3 id="important-workshop-dates">Important workshop dates</h3>
<ul>
  <li><strong>Submission deadline:</strong> March 22nd, 11:59 PM Pacific Time</li>
  <li><strong>Notification of acceptance:</strong> April 7th, 11:59 PM Pacific Time</li>
  <li><strong>Camera Ready submission deadline:</strong> April 14th, 11:59 PM Pacific Time</li>
  <li><strong>Workshop date:</strong> June 17th, 2024 (Morning)</li>
</ul>

<!-- We cordially invite submissions and participation in our “Backdoors in Deep Learning: The Good, the Bad, and the Ugly” workshop (neurips2023-bugs.github.io) that will be held on December 15 or 16, 2023 at NeurIPS 2023, New Orleans, USA.  -->

<!-- The submission deadline is **<s>September 29, 2023</s> October 6th, 2023, 23:59 AoE** and the submission link <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>.

#### Motivation and Topics

The main objective of the SyntaGen workshop is to offer a space for researchers, practitioners, and
enthusiasts to investigate, converse, and cooperate on the development, use, and potential uses of
synthetic visual datasets made from generative models. The workshop will cover various topics,
including but not restricted to:

* Leveraging pre-trained generative models to generate data and annotations for perception-driven tasks, including image classification, object detection, semantic and instance segmentation, relationship detection, action recognition, object tracking, and 3D shape reconstruction and recognition.
* Extending the generative capacity of large-scale pre-trained text-to-image models to other
domains, such as videos and 3D spaces.
* Synergizing expansive synthetic datasets with minimally annotated real datasets to enhance
model performance across scenarios including unsupervised, semi-supervised, weakly supervised, and zero-shot/few-shot learning.
* Enhancing data quality and improving synthesis methodologies in the context of pre-trained
text-to-image (T2I), text-to-video (T2V), and text-to-3D models.
* Evaluating the quality and effectiveness of the generated datasets, particularly on metrics,
challenges, and open problems related to benchmarking synthetic visual datasets.
* Ethical implications of using synthetic annotated data, strategies for mitigating biases, and
ensuring responsible data generation and annotation practices.

We only consider submissions that haven’t been published in any peer-reviewed venue, including NeurIPS 2023 conference. **We allow dual submissions with other workshops or conferences. The workshop is non-archival and will not have any official proceedings**. All accepted papers will be allocated either a poster presentation or a talk slot.
 

### Submission Instructions

Papers should be submitted to OpenReview: <a href="https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS">https://openreview.net/group?id=NeurIPS.cc/2023/Workshop/BUGS</a>

Submitted papers should have up to 6 pages (excluding references, acknowledgments, or appendices). Please use the NeurIPS submission template provided at <a href="https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles">https://neurips.cc/Conferences/2023/PaperInformation/StyleFiles</a>.
Submissions must be anonymous following NeurIPS double-blind reviewing guidelines, NeurIPS Code of Conduct, and Code of Ethics. Accepted papers will be hosted on the workshop website but are considered non-archival and can be submitted to other workshops, conferences, or journals if their submission policy allows. -->

  </article>

</div>
      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2024 SyntaGen.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.

    
  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="http://localhost:4000/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
<script src="http://localhost:4000/assets/js/katex.js"></script>




<!-- Include custom icon fonts -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="http://localhost:4000/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-XXXXXXXXX', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
